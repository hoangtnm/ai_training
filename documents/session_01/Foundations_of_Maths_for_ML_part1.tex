\documentclass[]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{references.bib} 
\graphicspath{ {images/} }

%opening
\title{Foundations of Math for ML, part 1}
\author{Hoang Nhat Minh Tran \thanks{Green Global Information Technology JSC}}
\date{\today}

\begin{document}

\maketitle

% The introduction
\begin{abstract}
Machine learning is a set of powerful mathematical tools that enable us, to represent, interpret, and control the complex world around us.
However, even just the word mathematics makes some people feel uneasy and unwelcome to explore the topic.
The purpose of this session is to take you on a tour through the basic maths underlying these methods, focusing in particular on building your intuition rather than worrying too much about the details.
Thanks to the amazing machine learning community, it's actually possible to apply many powerful machine learning methods without understanding very much about the underpinning mathematics, by using open source libraries.
This is great, but problems can arise and without some sense of the language and meaning of the relevant maths, you can struggle to work out what's gone wrong or how to fix it.
The ideal outcome of this session is that it will give you the confidence and motivation to immediately dive into one of the hundreds of boolean applied machine learning courses already available online, and not be intimidated by the matrix notation or the calculus.
\end{abstract}

% The first section's topic
% Every section is a main idea for what you have learnt
\section{Introduction to Linear Algebra and to Mathematics for Machine Learning}

In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focusing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the adding up - the point is to appreciate what they do and how they work so that, when things go wrong or there are special cases, you can understand why and what to do.

\subsection{Learning Objectives}

\begin{itemize}
	\item Recall how machine learning and vectors and matrices are related
	\item Interpret how changes in the model parameters affect the quality of the fit to the training data
	\item Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space
	\item Use substitution / elimination to solve a fairly easy linear algebra problem
	\item Understand how to add vectors and multiply by a scalar number
\end{itemize}

\subsection{The relationship between machine learning, linear algebra, and vectors and matrices}

\subsubsection{Motivations for linear algebra}

We're going to look a bit more at the types of problems we might want to solve, and expose what Linear Algebra is and how it might help us to solve them.
The first problem I might think of is one of price discovery.

\begin{equation} \label{eq1}
\begin{split}
2a + 3b & = 8 \\
10a + 1b & = 13
\end{split}
\end{equation}

Say I go shopping on two occasions, and I buy apples and bananas, and the first time I buy two apples and three bananas and they cost eight Euros.
And the second time I buy say, ten apples and one banana, and the cost is 13 Euros.
And the \textbf{As} and the \textbf{Bs} here,
are the price of a \textbf{single apple} and a \textbf{single banana}.
And what I'm going to have to do is \textbf{solve} these what we call \textbf{\textit{simultaneous equations}} in order to discover the price of individual apples and bananas.

Now in the general case of lots of different types of items and lots of shopping trips, then finding out the prices might be quite hard. It might be quite \textit{difficult} to \textit{solve} all these equations \textit{by hand}. So, we might want a \textit{computer algorithm} to do it for us, in the general case. Now, this is an example of a \textit{Linear Algebra} problem.

\begin{equation} \label{eq2}
	\begin{bmatrix}
	2 & 3 \\
	10 & 1
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
	a \\
	b
	\end{bmatrix}
	=
	\begin{bmatrix}
	8 \\
	13
	\end{bmatrix}
\end{equation}

I have some \textbf{constant} \textit{linear coefficients} here, these numbers \textit{2, 10, 3, 1}, that relate the \textbf{input} variables A and B, to the \textbf{output} 8 and 13, that is if I think about a vector [a,b], that describes the prices of apples and bananas.

Looking at these different types of mathematical objects, and understanding what they are and how to work with them, these \textit{vectors} and \textit{matrices}. In fact, with neural networks and machine learning, we want the computer in effect \textbf{\textit{not only}} to fit the equation, \textbf{\textit{but also}} to figure out what equation to use. That's a highly inexact description really of what's going on, but it gives the right sort of flavor.

\subsubsection{Getting a handle on vectors}

\subsubsection{Exploring parameter space}

\subsubsection{Solving some simultaneous equations}

\subsection{Operations with vectors}

\section{Data Manipulation}

It is impossible to get anything done if we cannot manipulate data. Generally, there are two important things we need to do with data: (i) acquire it and (ii) process it once it is inside the computer.  There is no point in acquiring data if we do not even know how to store it, so let’s get our hands dirty first by playing with synthetic data. We will start by introducing the Tensor, PyTorch’s primary tool for storing and transforming data.  If you have worked with NumPy before, you will notice that Tensor are, by design, similar to NumPy’s multi-dimensional array.  However, they confer a few key advantages.  First, Tensor supports asynchronous computation on CPU, GPU, and distributed cloud architectures. Second,they provide support for automatic differentiation. These properties make Tensor indispensable for deep learning.

\subsection{Getting Started}

Throughout this section, we are aiming to get you up and running with the basic functionality. Do not worry if you do not understand all of the basic math, like element-wise operations or normal distributions. We begin by importing PyTorch.

\begin{minted}[frame=single]{python}
import torch
\end{minted}

Tensors represent (possibly multi-dimensional) arrays of numerical values. Tensors with one axis cor-respond (in math-speak) to \textit{vectors}. Tensors with two axes correspond to \textit{matrices}. For arrays with more than two axes, mathematicians do not have special names—they simply call them \textit{tensors}.

The simplest object we can create is a vector. To start, we can use \textbf{arange} to create a row vector with 12 consecutive integers.

\begin{minted}[frame=single]{python}
x = torch.arange(12)
x
\end{minted}

\begin{minted}[frame=single]{python}
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
\end{minted}

We can get the Tensor instance shape through the \textbf{size} property.

\begin{minted}[frame=single]{python}
x.size()
\end{minted}

\begin{minted}[frame=single]{python}
torch.Size([12])
\end{minted}

We use the \textbf{view} function to change the shape of one (possibly multi-dimensional) array, to another that contains the same number of elements. For example, we can transform the shape of our line vector \textbf{x} to (3,4), which contains the same values but interprets them as a matrix containing 3 rows and 4 columns. Note that although the shape has changed, the elements in \textbf{x} have not. Moreover, the size remains the same.

\begin{minted}[frame=single]{python}
x.view(3, 4)
x
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 0,  1,  2,  3],
	[ 4,  5,  6,  7],
	[ 8,  9, 10, 11]])
\end{minted}

Reshaping by manually specifying each of the dimensions can get annoying.  Once we know one of the dimensions, why should we have to perform the division our selves to determine the other? For example,above, to get a matrix with 3 rows, we had to specify that it should have 4 columns (to account for the12 elements). Fortunately, Tensor can automatically work out one dimension given the other. We can invoke this capability by placing-1for the dimension that we would like Tensor to automatically infer. In our case, instead of \textbf{x.view((3, 4))}, we could have equivalently used \textbf{x.view((-1, 4))} or \textbf{x.view((3, -1))}.

To create a Tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4) we can invoke:

\begin{minted}[frame=single]{python}
torch.zeros(2,3,4)
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[[0., 0., 0., 0.],
	 [0., 0., 0., 0.],
	 [0., 0., 0., 0.]],
	
	 [[0., 0., 0., 0.],
	 [0., 0., 0., 0.],
	 [0., 0., 0., 0.]]])
\end{minted}

We can create tensors with each element set to 1 works via

\begin{minted}[frame=single]{python}
torch.ones(2,3,4)
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[[1., 1., 1., 1.],
	 [1., 1., 1., 1.],
	 [1., 1., 1., 1.]],
	
	 [[1., 1., 1., 1.],
	 [1., 1., 1., 1.],
	 [1., 1., 1., 1.]]])
\end{minted}

We can also specify the value of each element in the desired Tensor by supplying a Python list containing the numerical values.

\begin{minted}[frame=single]{python}
x = torch.tensor([[2,1,4,3],
	          [1,2,3,4],
	          [4,3,2,1]])
x
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[2, 1, 4, 3],
	[1, 2, 3, 4],
	[4, 3, 2, 1]])
\end{minted}

In some cases, we will want to randomly sample the values of each element in the Tensor according to some known probability distribution. This is especially common when we intend to use the array as a parameter in a neural network. The following snippet creates an Tensor with a shape of (3,4). Each of its elements is randomly sampled in a normal distribution with zero mean and unit variance

\begin{minted}[frame=single]{python}
torch.rand(3,4)
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[0.3600, 0.7597, 0.2731, 0.1179],
	[0.7362, 0.0399, 0.5782, 0.5169],
	[0.2102, 0.8529, 0.0264, 0.3641]])
\end{minted}

\subsection{Operations}

Oftentimes, we want to apply functions to arrays. Some of the simplest and most useful functions are the element-wise functions. These operate by performing a single scalar operation on the corresponding elements of two arrays. We can create an element-wise function from any function that maps from the scalars to the scalars. In math notations we would denote such a function as $ f: \mathbb{R} \rightarrow \mathbb{R} $. Given any two vectors \textbf{u} and \textbf{v} of the \textit{same} shape, and the function f, we can produce a vector $ c=F(u;v) $ by setting $ c_i \leftarrow (u_i; v_i) $ for all $ i $. In PyTorch, the common standard arithmetic operators (+,-,/,*,**) have all been \textit{lifted} to element-wise operations for identically-shaped tensors of arbitrary shape. We can call element-wise operations on any two tensors of the same shape, including matrices.

\begin{minted}[frame=single]{python}
x = torch.tensor([1, 2, 4, 8])
y = torch.ones_like(x) * 2	# result has the same size
print(f'x = {x}')
print(f'x + y {x+y}')
print(f'x - y {x-y}')
print(f'x * y {x*y}')
print(f'x / y {x/y}')
\end{minted}

\begin{minted}[frame=single]{python}
x = tensor([1, 2, 4, 8])
x + y tensor([ 3,  4,  6, 10])
x - y tensor([-1,  0,  2,  6])
x * y tensor([ 2,  4,  8, 16])
x / y tensor([0, 1, 2, 4])
\end{minted}

In addition to computations by element, we can also perform matrix operations, like matrix multiplication using the $ matmul $ function. Next, we will perform matrix multiplication of $ x $ and the transpose of $ y $. We define $ x $ as a matrix of 3 rows and 4 columns, and $ y $ is transposed into a matrix of 4 rows and 3 columns. The two matrices are multiplied to obtain a matrix of 3 rows and 3 columns.

\begin{minted}[frame=single]{python}
x = torch.arange(12).view(3,4)
y = torch.tensor([[2,1,4,3], [1,2,3,4], [4,3,2,1]])
torch.matmul(x, y.T) 
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 18,  20,  10],
	[ 58,  60,  50],
	[ 98, 100,  90]])
\end{minted}

We can also merge multiple Tensors. For that, we need to tell the system along which dimension to merge. The example below merges two matrices along dimension 0 (along rows) and dimension 1 (along columns)respectively

\begin{minted}[frame=single]{python}
torch.cat((x, y), dim=0)
torch.cat((x, y), dim=1)
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 0,  1,  2,  3,  2,  1,  4,  3],
	[ 4,  5,  6,  7,  1,  2,  3,  4],
	[ 8,  9, 10, 11,  4,  3,  2,  1]])
\end{minted}

Sometimes, we may want to construct binary Tensors via logical statements. Take $ x $ == $ y $ as an example. If $ x $ and $ y $ are equal for some entry, the new Tensor has a value of 1 at the same position; otherwise it is 0

\begin{minted}[frame=single]{python}
x == y
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[False,  True, False,  True],
	[False, False, False, False],
	[False, False, False, False]])
\end{minted}

Summing all the elements in the Tensor yields an Tensor with only one element.

\begin{minted}[frame=single]{python}
x.sum()
\end{minted}

\begin{minted}[frame=single]{python}
tensor(66)
\end{minted}

We can transform the result into a scalar in Python using  the asscalar function. In the following example,the $ l_2 $ norm of $ x $ yields a single element Tensor. The final result is transformed into a scalar.

\begin{minted}[frame=single]{python}
x = torch.arange(12, dtype=torch.float32).view(3,4)
x.norm().item()
\end{minted}

\begin{minted}[frame=single]{python}
22.494443893432617
\end{minted}

\subsection{Broadcast Mechanism}

In the above section, we saw how to perform operations on two Tensors of the same shape. When their shapes differ, a broadcasting mechanism may be triggered analogous to NumPy: first, copy the elements appropriately so that the two Tensors have the same shape, and then carry out operations by element.

\begin{minted}[frame=single]{python}
a = torch.arange(3).view((3,1))
b = torch.arange(2).view((1,2))
a, b
\end{minted}

\begin{minted}[frame=single]{python}
(tensor([[0],
	 [1],
	 [2]]), tensor([[0, 1]]))
\end{minted}

Since $ a $ and $ b $ are (3x1) and (1x2) matrices respectively, their shapes do not match up if we want to add them. Tensor addresses this by \textbf{\textit{broadcasting}} the entries of both matrices into a larger (3x2) matrix as follows: for matrix $ a $ it replicates the columns, for matrix $ b $ it replicates the rows before adding up both element-wise.

\begin{minted}[frame=single]{python}
a + b
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[0, 1],
	[1, 2],
	[2, 3]])
\end{minted}

\subsection{Indexing and Slicing}

Just like in any other Python array, elements in an Tensor can be accessed by its index. In good Python tradition the first element has index 0 and ranges are specified to include the first but not the last element.By this logic $ 1:3 $ selects the second and third element. Let’s try this out by selecting the respective rows in a matrix.

\begin{minted}[frame=single]{python}
x[1:3]
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 0.,  1.,  2.,  3.],
	[ 4.,  5.,  6.,  7.],
	[ 8.,  9., 10., 11.]])
\end{minted}

Beyond reading, we can also write elements of a matrix.

\begin{minted}[frame=single]{python}
x[1, 2] = 9
x
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 0.,  1.,  2.,  3.],
	[ 4.,  5.,  9.,  7.],
	[ 8.,  9., 10., 11.]])
\end{minted}

If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, $ [0:2, :] $ accesses the first and second rows. While we discussed indexing for matrices,this obviously also works for vectors and for tensors of more than 2 dimensions.

\begin{minted}[frame=single]{python}
x[0:2, :] = 12
x
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[12., 12., 12., 12.],
	[12., 12., 12., 12.],
	[ 8.,  9., 10., 11.]])
\end{minted}

\subsection{Mutual Transformation of NDArray and NumPy}

Converting PyTorch Tensors to and from NumPy is easy. The converted arrays do \textit{not share} memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or one of the GPUs, you do not want PyTorch having to wait whether NumPy might want to be doing something else with the same chunk of memory. The \textit{from\textunderscore numpy} and \textit{numpy} functions do the trick.

\begin{minted}[frame=single]{python}
import torch
import numpy as np

a = x.numpy()
print(type(a))
b = torch.from_numpy(a)
print(type(b))
\end{minted}

\begin{minted}[frame=single]{python}
<class 'numpy.ndarray'>
<class 'torch.Tensor'>
\end{minted}

\subsection{Exercise}

\begin{enumerate}
	\item Run the code in this section. Change the conditional statement $ x == y $ in this section to $ x < y $ or $ x > y $, and then see what kind of Tensor you can get.
	\item Replace the two Tensors that operate by element in the \textit{broadcast mechanism} with other shapes, e.g. three dimensional (3D) tensors. Is the result the same as expected?
	\item Rewrite $ c = torch.matmul(a, b.T) + c $ in the most memory efficient manner.
\end{enumerate}

\section{Recap}

% You just trained your first machine learning model. We saw that by training the model with input data and the corresponding output, the model learned to multiply the input by 1.8 and then add 32 to get the correct result.

\nocite{*}

\printbibliography

\end{document}