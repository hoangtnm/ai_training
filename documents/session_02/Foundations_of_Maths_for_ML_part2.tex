\documentclass[]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{references.bib} 
\graphicspath{ {images/} }

%opening
\title{Foundations of Math for ML, part 2}
\author{Hoang Nhat Minh Tran \thanks{Green Global Information Technology JSC}}
\date{\today}

\begin{document}

\maketitle

% The introduction
\begin{abstract}

\end{abstract}

\section{Linear Algebra}

Now that you can store and manipulate data, let’s briefly review the subset of basic linear algebra that you will need to understand most of the models. We will introduce all the basic concepts, the corresponding mathematical notation, and their realization in code all in one place. If you are already confident in your basic linear algebra, feel free to skim through or skip this chapter.

\begin{minted}[frame=single]{python}
import torch
\end{minted}

\subsection{Scalars}

If you never studied linear algebra or machine learning, you are probably used to working with one number at a time. And know how to do basic things like add them together or multiply them. For example, in PaloAlto, the temperature is 52 degrees Fahrenheit. Formally, we call these values \textit{scalars}. If you wanted to convert this value to Celsius (using metric system’s more sensible unit of temperature measurement), you would evaluate the expression $ f = c \times \frac{9}{5} + 32 $ setting $ f $ to 52. In this equation, each of the terms 32, 5, and 9 is a scalar value. The placeholders $ c $ and $ f $ that we use are called variables and they represent unknown scalar values.

In mathematical notation, we represent scalars with ordinary lower-cased letters $ (x,y,z) $. We also denote the space of all scalars as $ \mathbb{R} $. For expedience, we are going to punt a bit on what precisely a space is, but for now, remember that if you want to say that $ x $ is a scalar, you can simply say $ x \in \mathbb{R} $. The symbol $ \in $ can be pronounced “in” and just denotes membership in a set.

In PyTorch, we work with scalars by creating Tensors with just one element. In this snippet, we instantiate two scalars and perform some familiar arithmetic operations with them, such as addition, multiplication,division and exponentiation.

\begin{minted}[frame=single]{python}
x = torch.tensor([3.0])
y = torch.tensor([2.0])
print(f'x + y = {x+y}')
print(f'x * y = {x*y}')
print(f'x / y = {x/y}')
print(f'x ** y = {torch.pow(x,y)}')
\end{minted}

\begin{minted}[frame=single]{python}
x + y = tensor([5.])
x * y = tensor([6.])
x / y = tensor([1.5000])
x ** y = tensor([9.])
\end{minted}

We can convert any Tensor to a Python float by calling its \textit{numpy} method. Note that this is typically a bad idea. While you are doing this, Tensor has to stop doing anything else in order to hand the result and the process control back to Python. And unfortunately, Python is not very good at doing things in parallel.

So avoid sprinkling this operation liberally throughout your code or your networks will take a long time to train.

\begin{minted}[frame=single]{python}
x.numpy()
\end{minted}

\begin{minted}[frame=single]{python}
array([3.], dtype=float32)
\end{minted}

\subsection{Vectors}

You can think of a vector as simply a list of numbers, for example $ [1.0,3.0,4.0,2.0] $. Each of the numbers in the vector consists of a single scalar value. We call these values the \textit{entries} or \textit{components} of the vector.Often, we are interested in vectors whose values hold some real-world significance. For example, if we are studying the risk that loans default, we might associate each applicant with a vector whose components correspond to their income, length of employment, number of previous defaults, etc. If we were studying the risk of heart attacks hospital patients potentially face, we might represent each patient with a vector whose components capture their most recent vital signs, cholesterol levels, minutes of exercise per day, etc.In math notation, we will usually denote vectors as bold-faced, lower-cased letters (\textbf{u}, \textbf{v}, \textbf{w}). In PyTorch, we work with vectors via 1D Tensors with an arbitrary number of components.

\begin{minted}[frame=single]{python}
x = torch.arange(4)
print(f'x = {x}')
\end{minted}

\begin{minted}[frame=single]{python}
x = tensor([0, 1, 2, 3])
\end{minted}

We can refer to any element of a vector by using a subscript. For example, we can refer to the 4th element of \textbf{u} by $ u_4 $. Note that the element $ u_4 $ is a scalar, so we do not bold-face the font when referring to it. Incode, we access any element $ i $ by indexing into the Tensor.

\begin{minted}[frame=single]{python}
x[3]
\end{minted}

\begin{minted}[frame=single]{python}
tensor(3)
\end{minted}

\subsection{Length, dimensionality and shape}

Let’s revisit some concepts from the previous section. A vector is just an array of numbers. And just as every array has a length, so does every vector. In math notation, if we want to say that a vector $ x $ consists of $ n $ real-valued scalars, we can express this as $ x \in \mathbb{R}^n $.  The length of a vector is commonly called its \textit{dimension}. As with an ordinary Python array, we can access the length of an Tensor by calling Python’s in-built len()function.

We can also access a vector’s length via its .size method. The shape is a tuple that lists the dimensionalityof the NDArray along each of its axes. Because a vector can only be indexed along one axis, its shape hasjust one element.

\begin{minted}[frame=single]{python}
x.size()
\end{minted}

\begin{minted}[frame=single]{python}
torch.Size([4])
\end{minted}

\subsection{Matrices}

Just as vectors generalize scalars from order 0 to order 1, matrices generalize vectors from 1D to 2D. Matrices, which we’ll typically denote with capital letters (A,B,C), are represented in code as arrays with2 axes. Visually, we can draw a matrix as a table, where each entry $ a_ij $ belongs to the $ i $-th row and $ j $-th column.

\begin{equation} \label{}
A = \\
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{bmatrix}
\end{equation}

We can create a matrix with $ n $ rows and $ m $ columns in PyTorch by specifying a shape with two components $ (n,m) $ when calling any of our favorite functions for instantiating an tensor such as ones, or zeros.

\begin{minted}[frame=single]{python}
A = torch.arange(20).view(5,4)
print(A)
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 0,  1,  2,  3],
	[ 4,  5,  6,  7],
	[ 8,  9, 10, 11],
	[12, 13, 14, 15],
	[16, 17, 18, 19]])
\end{minted}

Matrices are useful data structures: they allow us to organize data that has different modalities of variation. For example, rows in our matrix might correspond to different patients, while columns might correspond to different attributes.

We can access the scalar elements $ a_{ij} $ of a matrix $ A $ by specifying the indices for the row ($ i $) and column ($ j $) respectively. Leaving them blank via a : takes all elements along the respective dimension (as seen in the previous section).

We can transpose the matrix through T. That is, if $ B = A^T $, then $ b_{ij} = a_{ji} $ for any $ i $ and $ j $.

\begin{minted}[frame=single]{python}
print(A.T)
\end{minted}

\begin{minted}[frame=single]{python}
tensor([[ 0,  4,  8, 12, 16],
	[ 1,  5,  9, 13, 17],
	[ 2,  6, 10, 14, 18],
	[ 3,  7, 11, 15, 19]])
\end{minted}

\subsection{Tensors}

Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures with even more axes. Tensors give us a generic way of discussing arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors.

Using tensors will become more important when we start working with images, which arrive as 3D data structures, with axes corresponding to the height, width, and the three (RGB) color channels. But in this document, we’re going to skip this part and make sure you know the basics.

\begin{minted}[frame=single]{python}
X = torch.arange(24).view(2,3,4)
print(f'X.shape = {X.size()}')
print(f'X = {X}')
\end{minted}

\begin{minted}[frame=single]{python}
X.shape = torch.Size([2, 3, 4])
X = tensor([[[ 0,  1,  2,  3],
	     [ 4,  5,  6,  7],
	     [ 8,  9, 10, 11]],
	
	    [[12, 13, 14, 15],
	     [16, 17, 18, 19],
	     [20, 21, 22, 23]]])
\end{minted}

\subsection{Sums and means}

The next more sophisticated thing we can do with arbitrary tensors is to calculate the sum of their elements. In mathematical notation, we express sums using the $ \Sigma $ symbol. To express the sum of the elements in a vector \textbf{u} of length \textit{d}, we can write $ \sum_{i=1}^{d} u_i $. In code, we can just call torch.sum().

\begin{minted}[frame=single]{python}
x = torch.ones(3)
print(x)
print(torch.sum(x))
\end{minted}

\begin{minted}[frame=single]{python}
tensor([1., 1., 1.])
tensor(3.)
\end{minted}

A related quantity is the mean, which is also called the \textit{average}. We calculate the mean by dividing the sum by the total number of elements. With mathematical notation, we could write the average over a vector \textbf{u} as $  \frac{1}{d} \sum_{i=1}^{d} u_i $ and the average over a matrix A as $ \frac{1}{n \times m} \sum_{i=1}^{m} \sum_{j=1}^{1} a_{ij} $. In code, we could just call torch.mean() on tensors of arbitrary shape:

\begin{minted}[frame=single]{python}
A = torch.arange(20, dtype=torch.float32).view(5,4)
print(torch.mean(A))
\end{minted}

\begin{minted}[frame=single]{python}
tensor(9.5000)
\end{minted}

\subsection{Dot products}

So far, we have only performed element-wise operations, sums and averages. And if this was all we could do, linear algebra probably would not deserve its own document.  However, one of the most fundamental operations is the dot product. Given two vectors \textbf{u} and \textbf{v}, the dot product $ u^T v $ is a sum over the products of the corresponding elements: $ u^T v = \sum_{i=1}^{d} u_i . v_i $.

\begin{minted}[frame=single]{python}
x = torch.arange(4, dtype=torch.float32)
y = torch.ones(4, dtype=torch.float32)
print(x, y, torch.matmul(x, y))
\end{minted}

\begin{minted}[frame=single]{python}
tensor([0., 1., 2., 3.]) tensor([1., 1., 1., 1.]) tensor(6.)
\end{minted}

Note that we can express the dot product of two vectors \textit{torch.matmul(x, y)} equivalently by performing an element-wise multiplication and then a sum:

\begin{minted}[frame=single]{python}
torch.sum(x*y)
\end{minted}

\begin{minted}[frame=single]{python}
tensor(6.)
\end{minted}

% \subsection{Matrix-vector products}

% \subsection{Matrix-matrix multiplication}

% \subsection{Norms}

\subsection{Norms and objectives}

While we do not want to get too far ahead of ourselves, we do want you to anticipate why these concepts are useful. In machine learning we are often trying to solve optimization problems: \textit{Maximize} the probability assigned to observed data. \textit{Minimize} the distance between predictions and the ground-truth observations.Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized.  Oftentimes, these objectives, perhaps the most important component of a machine learning algorithm (besides the data itself),are expressed as norm

\subsection{Intermediate linear algebra}

If you have made it this far, and understand everything that we have covered, then honestly, you \textit{are} ready to begin modeling. If you are feeling antsy, this is a perfectly reasonable place to move on. You already know nearly all of the linear algebra required to implement a number of many practically useful models andyou can always circle back when you want to learn more.

But there is a lot more to linear algebra, even as concerns machine learning. At some point, if you plan to make a career in machine learning, you will need to know more than what we have covered so far. In the rest of this document, we introduce some useful, more advanced concepts.

\subsubsection{Basic vector properties}

Vectors are useful beyond being data structures to carry numbers. In addition to reading and writing values to the components of a vector, and performing some useful mathematical operations, we can analyze vectors in some interesting ways.

One important concept is the notion of a vector space. Here are the conditions that make a vector space:

\begin{itemize}
	\item \textbf{Additive axioms} (we assume that x, y, z are all vectors): $ x+y=y+x $ and $ (x + y) + z = x + (y+z) $ and $ 0 +x=x+ 0 =x $ and $ (-x) + x = x + (-x) =0 $.
	\item \textbf{Multiplicative axioms} (we assume that x is a vector and a, b are scalars): $ 0.x= 0 $ and $ 1.x=x $ and $ (ab)x=a(bx) $.
	\item \textbf{Distributive axioms} (we assume that x and y are vectors and a, b are scalars): $ a(x+y) = ax+ay $ and $ (a+b)x = ax + bx $
\end{itemize}

\subsection{Summary}

In just a few pages (or one Jupyter notebook) we have taught you all the linear algebra you will need to understand a good chunk of neural networks. Of course there is a lot more to linear algebra. And a \textins{lot} of that math \textit{is} useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensional structure in real-world datasets. There are entire sub fields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning.And we believe you will be much more inclined to learn more mathematics once you have gotten your hands dirty deploying useful machine learning models on real datasets. So while we reserve the right to introduce more math much later on, we will wrap up this chapter here.

\section{Recap}

% You just trained your first machine learning model. We saw that by training the model with input data and the corresponding output, the model learned to multiply the input by 1.8 and then add 32 to get the correct result.

\nocite{*}

\printbibliography

\end{document}